import gradio as gr
import os
import tempfile
from PIL import Image
import time
import numpy as np
from inference import ModelInference
from dswrapper import generate_social_content

# é…ç½®æ¨¡å‹è·¯å¾„
BASE_MODEL = "ybelkada/blip2-opt-2.7b-fp16-sharded"
ADAPTER_PATH = "blip2-finetuned"
EMOTION_MODEL_PATH = "best_model.pt"

# åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨ä¸Šä¼ çš„å›¾ç‰‡
TEMP_DIR = tempfile.mkdtemp()
print(f"ä¸´æ—¶ç›®å½•åˆ›å»ºåœ¨: {TEMP_DIR}")

# åˆ›å»ºoffloadç›®å½•ç”¨äºæ¨¡å‹å¸è½½
OFFLOAD_DIR = os.path.join(TEMP_DIR, "offload")
os.makedirs(OFFLOAD_DIR, exist_ok=True)

class EmotiGramApp:
    def __init__(self):
        self.inferencer = None
        # ç›´æ¥åˆå§‹åŒ–æ¨¡å‹
        self.load_models_on_start()
    
    def load_models_on_start(self):
        """å¯åŠ¨æ—¶åŠ è½½æ‰€æœ‰æ¨¡å‹ï¼Œè€Œä¸æ˜¯é€šè¿‡æŒ‰é’®è§¦å‘"""
        try:
            print("å¼€å§‹åŠ è½½æ‰€æœ‰æ¨¡å‹...")
            start_time = time.time()
            
            # åˆå§‹åŒ–æ¨ç†ç±»
            self.inferencer = ModelInference(offload_dir=OFFLOAD_DIR)
            
            # 1. å…ˆåŠ è½½å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹
            multi_modal_loaded = self.inferencer.load_multi_modal_model(
                model_path=EMOTION_MODEL_PATH
            )
            if not multi_modal_loaded:
                print("âŒ å¤šæ¨¡æ€æƒ…æ„Ÿæ¨¡å‹åŠ è½½å¤±è´¥")
                return False
            
            # 2. åŠ è½½åŸºç¡€æ¨¡å‹
            base_loaded = self.inferencer.load_base_model(
                base_model=BASE_MODEL
            )
            if not base_loaded:
                print("âŒ åŸºç¡€æ¨¡å‹åŠ è½½å¤±è´¥")
                return False
                
            elapsed = time.time() - start_time
            print(f"âœ… æ‰€æœ‰æ¨¡å‹åŠ è½½æˆåŠŸï¼è€—æ—¶: {elapsed:.2f} ç§’")
            return True
            
        except Exception as e:
            import traceback
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            print(traceback.format_exc())
            return False
    
    def process_image(self, image):
        """å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡å¹¶è¿”å›ç»“æœ"""
        if self.inferencer is None:
            return "æ¨¡å‹å°šæœªåˆå§‹åŒ–ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•", "", "", "", ""
        
        if image is None:
            return "è¯·ä¸Šä¼ å›¾ç‰‡", "", "", "", ""
        
        # ä¿å­˜ä¸Šä¼ çš„å›¾ç‰‡åˆ°ä¸´æ—¶æ–‡ä»¶
        if isinstance(image, np.ndarray):
            pil_image = Image.fromarray(image)
        else:
            pil_image = Image.open(image)
            
        temp_image_path = os.path.join(TEMP_DIR, f"upload_{int(time.time())}.jpg")
        pil_image.save(temp_image_path)
        
        try:
            # ä½¿ç”¨åŸå§‹BLIP-2æ¨¡å‹ç”Ÿæˆæè¿°
            start_time = time.time()
            raw_caption = self.inferencer.generate_caption(temp_image_path, use_adapter=False)
            raw_time = time.time() - start_time
            print(f"åŸå§‹æè¿°ç”Ÿæˆè€—æ—¶: {raw_time:.2f}ç§’")
            
            # ä½¿ç”¨å¾®è°ƒBLIP-2ç”Ÿæˆæè¿°
            start_time = time.time()
            tuned_caption = self.inferencer.generate_caption(temp_image_path, use_adapter=True)
            tuned_time = time.time() - start_time
            print(f"é£æ ¼åŒ–æè¿°ç”Ÿæˆè€—æ—¶: {tuned_time:.2f}ç§’")
            
            # ä½¿ç”¨æƒ…æ„Ÿæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»
            start_time = time.time()
            emotion_label, emotion_probs = self.inferencer.predict_emotion(temp_image_path, tuned_caption)
            emotion_time = time.time() - start_time
            print(f"æƒ…æ„Ÿåˆ†æè€—æ—¶: {emotion_time:.2f}ç§’")
            
            # è·å–æƒ…æ„Ÿæ¦‚ç‡çš„æ ¼å¼åŒ–å­—ç¬¦ä¸²
            probs_str = ", ".join([f"{label}: {prob*100:.2f}%" 
                                 for label, prob in zip(["negative", "neutral", "positive"], emotion_probs)])
            
            # è°ƒç”¨DeepSeekç”Ÿæˆç¤¾äº¤åª’ä½“å†…å®¹
            start_time = time.time()
            social_content = generate_social_content(raw_caption, tuned_caption, emotion_label)
            deepseek_time = time.time() - start_time
            print(f"DeepSeekæ–‡æ¡ˆç”Ÿæˆè€—æ—¶: {deepseek_time:.2f}ç§’")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                os.remove(temp_image_path)
            except:
                pass
                
            return (
                f"ğŸ“ åŸå§‹æè¿°ï¼š\n{raw_caption}",
                f"ğŸ“¢ é£æ ¼åŒ–æè¿°ï¼š\n{tuned_caption}",
                f"ğŸ­ æƒ…æ„Ÿåˆ†æï¼š{emotion_label}",
                f"æƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒï¼š{probs_str}",
                f"ğŸŒŸ ç¤¾äº¤åª’ä½“æ–‡æ¡ˆå»ºè®®ï¼š\n{social_content}"
            )
            
        except Exception as e:
            import traceback
            error_msg = traceback.format_exc()
            return f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™ï¼š{str(e)}\n\n{error_msg}", "", "", "", ""

    def create_interface(self):
        """åˆ›å»ºGradioç•Œé¢"""
        with gr.Blocks(title="EmotiGram - æƒ…æ„Ÿå›¾æ–‡ç”Ÿæˆå™¨") as interface:
            gr.Markdown("# ğŸ“¸ EmotiGram - æ™ºèƒ½å›¾æ–‡æƒ…æ„Ÿç”Ÿæˆå™¨")
            gr.Markdown("ä¸Šä¼ å›¾ç‰‡ï¼Œè·å–AIç”Ÿæˆçš„å›¾åƒæè¿°ã€æƒ…æ„Ÿåˆ†æå’Œç¤¾äº¤åª’ä½“æ–‡æ¡ˆå»ºè®®ã€‚")
            
            with gr.Row():
                with gr.Column(scale=1):
                    upload_image = gr.Image(label="ä¸Šä¼ å›¾ç‰‡")
                    process_button = gr.Button("å¤„ç†å›¾ç‰‡", variant="primary")
                    
                    # æ·»åŠ çŠ¶æ€æŒ‡ç¤º
                    status = gr.Textbox(label="å¤„ç†çŠ¶æ€", value="æ¨¡å‹å·²åŠ è½½ï¼Œå¯ä»¥å¼€å§‹å¤„ç†å›¾ç‰‡")
                
                with gr.Column(scale=2):
                    raw_caption = gr.Textbox(label="åŸå§‹æè¿°", lines=3)
                    tuned_caption = gr.Textbox(label="é£æ ¼åŒ–æè¿°", lines=3)
                    emotion_label = gr.Textbox(label="æƒ…æ„Ÿåˆ†æç»“æœ", lines=1)
                    emotion_probs = gr.Textbox(label="æƒ…æ„Ÿæ¦‚ç‡åˆ†å¸ƒ", lines=1)
                    social_content = gr.Textbox(label="ç¤¾äº¤åª’ä½“æ–‡æ¡ˆå»ºè®®", lines=6)
            
            # è®¾ç½®äº‹ä»¶å¤„ç†
            process_button.click(
                fn=self.process_image, 
                inputs=[upload_image], 
                outputs=[raw_caption, tuned_caption, emotion_label, emotion_probs, social_content]
            )
            
            gr.Markdown("""
            ## ğŸ“‹ ä½¿ç”¨è¯´æ˜
            1. ä¸Šä¼ ä¸€å¼ å›¾ç‰‡
            2. ç‚¹å‡»"å¤„ç†å›¾ç‰‡"æŒ‰é’®
            3. æŸ¥çœ‹ç”Ÿæˆçš„æè¿°ã€æƒ…æ„Ÿåˆ†æå’Œç¤¾äº¤åª’ä½“æ–‡æ¡ˆå»ºè®®
            
            ## âš™ï¸ æŠ€æœ¯ç»†èŠ‚
            - ä½¿ç”¨BLIP-2è¿›è¡Œå›¾åƒæè¿°ç”Ÿæˆ
            - ä½¿ç”¨å¾®è°ƒçš„BLIP-2ç”Ÿæˆé£æ ¼åŒ–æè¿°
            - ä½¿ç”¨å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹é¢„æµ‹å›¾åƒå’Œæ–‡æœ¬çš„æƒ…æ„Ÿ
            - ä½¿ç”¨DeepSeek APIç”Ÿæˆç¤¾äº¤åª’ä½“æ–‡æ¡ˆå»ºè®®
            """)
            
        return interface

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    app = EmotiGramApp()
    demo = app.create_interface()
    demo.launch(share=True)