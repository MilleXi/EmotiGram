from inference import ModelInference
from PIL import Image

# è·¯å¾„é…ç½®
image_path = "data/image/1313.jpg"

# åˆå§‹åŒ–æ¨ç†ç±»
inferencer = ModelInference()

# 1. é¦–å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
base_loaded = inferencer.load_base_model(
    base_model="ybelkada/blip2-opt-2.7b-fp16-sharded"
)

if not base_loaded:
    print("åŸºç¡€æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„å’Œä¾èµ–")
    exit()

# 2. ä½¿ç”¨åŸå§‹BLIP-2æ¨¡å‹ç”Ÿæˆæè¿°
raw_caption = inferencer.generate_caption(image_path)
print(f"\nğŸ“ æœªå¾®è°ƒBLIP-2ç”ŸæˆåŸå§‹æè¿°ï¼š\n{raw_caption}")

# 3. åº”ç”¨é€‚é…å™¨
adapter_applied = inferencer.apply_adapter(adapter_path="blip2-finetuned")

if not adapter_applied:
    print("é€‚é…å™¨åº”ç”¨å¤±è´¥")
    exit()

# 4. ä½¿ç”¨å¾®è°ƒBLIP-2ç”Ÿæˆæè¿°
tuned_caption = inferencer.generate_caption(image_path)
print(f"\nğŸ“¢ å¾®è°ƒBLIP-2ç”Ÿæˆæ¨æ–‡æè¿°ï¼š\n{tuned_caption}")

# 5. åŠ è½½å¤šæ¨¡æ€æƒ…æ„Ÿåˆ†ææ¨¡å‹
multi_modal_loaded = inferencer.load_multi_modal_model(
    model_path="best_model.pt"
)

if not multi_modal_loaded:
    print("å¤šæ¨¡æ€æ¨¡å‹åŠ è½½å¤±è´¥")
    exit()

# 6. ä½¿ç”¨æƒ…æ„Ÿæ¨¡å‹è¿›è¡Œæƒ…æ„Ÿåˆ†ç±»
label, probs = inferencer.predict_emotion(image_path, tuned_caption)
print(f"\nğŸ­ å¤šæ¨¡æ€æƒ…æ„Ÿé¢„æµ‹ç»“æœï¼š{label}")
print(f"ç±»åˆ«æ¦‚ç‡ï¼š{probs}")